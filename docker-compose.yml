version: '3.9'

x-base_service: &base_service
    ports:
      - "7860:7860"
    volumes:
      - &v1 ./data:/data
      - &v2 /richfast-jfs/sd/output:/output
      - &v3 /richfast-jfs/sd/input:/input
      - &v4 ./data-dreambooth:/stable-diffusion-webui/models/dreambooth
      - &v5 ./data-dreambooth:/dreambooth-output
    deploy:
      resources:
        reservations:
          devices:
              - driver: nvidia
                count: all
                capabilities: [gpu]

name: webui-docker

services:
  download:
    build: ./services/download/
    profiles: ["download"]
    volumes:
      - *v1

  auto-t4:
    <<: *base_service
    profiles: ["auto-t4"]
    image: brandnewx/automatic1111
    environment:
      - CLI_ARGS=--allow-code --medvram --xformers --force-enable-xformers --opt-split-attention --enable-insecure-extension-access --administrator --disable-safe-unpickle
      
  auto: &automatic
    <<: *base_service
    profiles: ["auto"]
    image: brandnewx/automatic1111
    environment:
      - CLI_ARGS=--allow-code --opt-split-attention --enable-insecure-extension-access --administrator --disable-safe-unpickle
      
  dreambooth: &dreambooth
    <<: *base_service
    profiles: ["dreambooth"]
    image: brandnewx/dreambooth
    restart: "no"
    environment:
      - INSTANCE_DIR=${INSTANCE_DIR:-/input/skssks}
      - MODEL_NAME=${MODEL_NAME:-skssks}
      - OUTPUT_DIR=/dreambooth-output
      - MODEL_PATH=${MODEL_PATH-}
      - MAX_TRAIN_STEPS=${MAX_TRAIN_STEPS:-2000}
      - TEXT_ENCODER_STEPS=${TEXT_ENCODER_STEPS:-500}
      - SAVE_STARTING_STEPS=${SAVE_STARTING_STEPS:-1500}
      - SAVE_N_STEPS=${SAVE_N_STEPS:-500}
      - SEED=${SEED:-1337}
      - KEEP_DIFFUSERS_MODEL=${KEEP_DIFFUSERS_MODEL:-0}
      - SAVE_INTERMEDIARY_DIRS=${SAVE_INTERMEDIARY_DIRS:-0}
      
  auto-cpu:
    <<: *automatic
    profiles: ["auto-cpu"]
    deploy: {}
    environment:
      - CLI_ARGS=--no-half --precision full

  hlky:
    <<: *base_service
    profiles: ["hlky"]
    build: ./services/hlky/
    image: sd-hlky:8
    environment:
      - CLI_ARGS=--optimized-turbo
      - USE_STREAMLIT=0

  lstein:
    <<: *base_service
    profiles: ["lstein"]
    build: ./services/lstein/
    image: sd-lstein:6
    environment:
      - PRELOAD=true
      - CLI_ARGS=--max_loaded_models=1
